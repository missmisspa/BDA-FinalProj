{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1cTi6iIYqoCE",
        "outputId": "700a2293-1ad0-4162-f7d0-cd34ef9e1a37"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Merge dataset pre-processing"
      ],
      "metadata": {
        "id": "PEwBBlTeQgWR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hrhAS6IMqdNQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "739a4408-6870-4f58-cd13-1d2e89e7a718"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Cleaned & Sorted Data Preview:\n",
            "+----+----+----+----+----------+---------+----+---+---+---+---+----+------+------------+-------------------+------------+--------+---------+\n",
            "|   c|   h|   l|   o|         t|        v|   y|  m|  d|  w| wd|last|change|     pchange|       Company Name|Stock Symbol|  Sector|Subsector|\n",
            "+----+----+----+----+----------+---------+----+---+---+---+---+----+------+------------+-------------------+------------+--------+---------+\n",
            "| 6.7| 6.7| 6.7| 6.7|10/12/2013|  53800.0|2013| 12| 10| 50|  1|6.66|  0.04| 0.006006006|8990 Holdings, Inc.|       HOUSE|Property| Property|\n",
            "|7.97|8.18|7.96|8.18|15/07/2014|3709800.0|2014|  7| 15| 29|  1| 8.0| -0.03|    -0.00375|8990 Holdings, Inc.|       HOUSE|Property| Property|\n",
            "| 5.4|5.54|5.28| 5.3| 3/11/2017|1213800.0|2017| 11|  3| 44|  4|5.33|  0.07| 0.013133208|8990 Holdings, Inc.|       HOUSE|Property| Property|\n",
            "|6.41|6.41| 6.1| 6.1|18/11/2013|   3400.0|2013| 11| 18| 47|  0|6.57| -0.16| -0.02435312|8990 Holdings, Inc.|       HOUSE|Property| Property|\n",
            "|6.98|6.98| 6.7| 6.7|27/03/2014|  62100.0|2014|  3| 27| 13|  3| 6.9|  0.08| 0.011594203|8990 Holdings, Inc.|       HOUSE|Property| Property|\n",
            "|7.38| 7.5|7.36|7.36|26/06/2015| 154300.0|2015|  6| 26| 26|  4| 7.4| -0.02|-0.002702703|8990 Holdings, Inc.|       HOUSE|Property| Property|\n",
            "| 7.2|7.35| 7.1|7.19| 3/10/2014|3108900.0|2014| 10|  3| 40|  4|7.14|  0.06| 0.008403361|8990 Holdings, Inc.|       HOUSE|Property| Property|\n",
            "| 7.4| 7.4| 6.2| 6.8|28/11/2013| 155300.0|2013| 11| 28| 48|  3| 6.8|   0.6| 0.088235294|8990 Holdings, Inc.|       HOUSE|Property| Property|\n",
            "| 7.6|7.98|7.55|7.55| 11/5/2016|4514200.0|2016|  5| 11| 19|  2|7.55|  0.05| 0.006622517|8990 Holdings, Inc.|       HOUSE|Property| Property|\n",
            "|7.17|7.23|6.63|6.64|18/09/2015|5780400.0|2015|  9| 18| 38|  4|6.64|  0.53| 0.079819277|8990 Holdings, Inc.|       HOUSE|Property| Property|\n",
            "|7.44|7.49|7.36|7.49|18/06/2015|  44200.0|2015|  6| 18| 25|  3|7.44|   0.0|           0|8990 Holdings, Inc.|       HOUSE|Property| Property|\n",
            "| 7.1| 7.1| 7.1| 7.1|21/02/2014|  12000.0|2014|  2| 21|  8|  4| 7.1|   0.0|           0|8990 Holdings, Inc.|       HOUSE|Property| Property|\n",
            "|6.99|6.99|6.91|6.95| 2/12/2016| 737600.0|2016| 12|  2| 48|  4|6.92|  0.07| 0.010115607|8990 Holdings, Inc.|       HOUSE|Property| Property|\n",
            "| 7.1|7.15|7.01|7.15| 10/2/2016|  87300.0|2016|  2| 10|  6|  2| 7.1|   0.0|           0|8990 Holdings, Inc.|       HOUSE|Property| Property|\n",
            "| 6.3|6.43|6.29|6.43|29/05/2017| 431500.0|2017|  5| 29| 22|  0|6.48| -0.18|-0.027777778|8990 Holdings, Inc.|       HOUSE|Property| Property|\n",
            "|6.51| 6.6| 6.5| 6.6|30/09/2015|  75700.0|2015|  9| 30| 40|  2|6.51|   0.0|           0|8990 Holdings, Inc.|       HOUSE|Property| Property|\n",
            "|5.35|5.38|5.33|5.37|22/08/2017| 520100.0|2017|  8| 22| 34|  1|5.37| -0.02|-0.003724395|8990 Holdings, Inc.|       HOUSE|Property| Property|\n",
            "|7.84| 7.9|7.84| 7.9|15/09/2016| 238600.0|2016|  9| 15| 37|  3|7.84|   0.0|           0|8990 Holdings, Inc.|       HOUSE|Property| Property|\n",
            "|7.16| 7.4| 7.1|7.39| 5/12/2014|3033800.0|2014| 12|  5| 49|  4|7.43| -0.27|-0.036339166|8990 Holdings, Inc.|       HOUSE|Property| Property|\n",
            "| 6.8| 6.9|6.75| 6.9|15/02/2017| 432100.0|2017|  2| 15|  7|  2| 7.0|  -0.2|-0.028571429|8990 Holdings, Inc.|       HOUSE|Property| Property|\n",
            "+----+----+----+----+----------+---------+----+---+---+---+---+----+------+------------+-------------------+------------+--------+---------+\n",
            "only showing top 20 rows\n",
            "\n",
            "✅ Cleaned and sorted dataset saved as 'final_merge_dataset.csv' in your Google Drive!\n"
          ]
        }
      ],
      "source": [
        "# Install PySpark if not already installed\n",
        "!pip install -q pyspark\n",
        "\n",
        "# Import libraries\n",
        "from pyspark.sql import SparkSession\n",
        "from google.colab import drive\n",
        "import os\n",
        "import shutil\n",
        "import glob\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "\n",
        "spark = SparkSession.builder.appName(\"Data Preprocessing\").getOrCreate()\n",
        "\n",
        "\n",
        "input_path = \"/content/drive/MyDrive/Big Data Final Project/Dataset/merge dataset/merged_data_w_sectors.csv\"\n",
        "temp_output_dir = \"/content/cleaned_temp_output\"\n",
        "final_dir = \"/content/drive/MyDrive/Big Data Final Project/Dataset/merge dataset/Cleaned_Dataset\"\n",
        "final_output_path = f\"{final_dir}/final_merge_dataset.csv\"\n",
        "\n",
        "\n",
        "df = spark.read.option(\"header\", \"true\").csv(input_path)\n",
        "\n",
        "# Drop 'symbol' and 'Listing Date' columns\n",
        "df_cleaned = df.drop(\"symbol\", \"Listing Date\")\n",
        "\n",
        "# Sort alphabetically by 'Company Name'\n",
        "df_sorted = df_cleaned.orderBy(\"Company Name\")\n",
        "\n",
        "# Show cleaned and sorted sample\n",
        "print(\"Cleaned & Sorted Data Preview:\")\n",
        "df_sorted.show()\n",
        "\n",
        "# Save the cleaned and sorted data\n",
        "os.makedirs(final_dir, exist_ok=True)\n",
        "\n",
        "# Write to a temporary output directory\n",
        "df_sorted.coalesce(1).write.mode(\"overwrite\").option(\"header\", \"true\").csv(temp_output_dir)\n",
        "\n",
        "# Move the actual part file to the final location and rename\n",
        "part_files = glob.glob(f\"{temp_output_dir}/part-*.csv\")\n",
        "if len(part_files) != 1:\n",
        "    raise ValueError(\"Expected exactly one part file, but found multiple or none.\")\n",
        "shutil.move(part_files[0], final_output_path)\n",
        "\n",
        "# Remove the temporary directory\n",
        "shutil.rmtree(temp_output_dir)\n",
        "\n",
        "print(\"✅ Cleaned and sorted dataset saved as 'final_merge_dataset.csv' in your Google Drive!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Final Preprocessing: stocks and data.csv"
      ],
      "metadata": {
        "id": "DoKNh9iVQdNb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install PySpark if not already installed\n",
        "!pip install -q pyspark\n",
        "\n",
        "# Import libraries\n",
        "from pyspark.sql import SparkSession\n",
        "from google.colab import drive\n",
        "import os\n",
        "import shutil\n",
        "import glob\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Initialize Spark session\n",
        "spark = SparkSession.builder.appName(\"Data Preprocessing\").getOrCreate()\n",
        "\n",
        "# Define paths\n",
        "base_path = \"/content/drive/MyDrive/Big Data Final Project/Dataset\"\n",
        "input_file = f\"{base_path}/dataset_with_NULL_values/data_with_nulls.csv\"\n",
        "output_folder = f\"{base_path}/Cleaned_Dataset\"\n",
        "final_output_path = f\"{output_folder}/cleaned_data_V2.csv\"\n",
        "temp_output_dir = f\"{output_folder}/temp_output\"\n",
        "final_dir = output_folder\n",
        "\n",
        "# Load dataset\n",
        "df = spark.read.option(\"header\", \"true\").csv(input_file)\n",
        "\n",
        "# Drop rows with null values\n",
        "df = df.dropna()\n",
        "\n",
        "# Drop unnecessary columns\n",
        "df_cleaned = df.drop(\"w\", \"wd\", \"last\", \"pchange\")\n",
        "\n",
        "# Sort alphabetically by 'symbol'\n",
        "df_sorted = df_cleaned.orderBy(\"symbol\")\n",
        "\n",
        "# Show cleaned and sorted sample\n",
        "print(\"Cleaned & Sorted Data Preview:\")\n",
        "df_sorted.show(10)\n",
        "\n",
        "# Save the cleaned and sorted data\n",
        "os.makedirs(final_dir, exist_ok=True)\n",
        "\n",
        "# Write to a temporary output directory\n",
        "df_sorted.coalesce(1).write.mode(\"overwrite\").option(\"header\", \"true\").csv(temp_output_dir)\n",
        "\n",
        "# Move the actual part file to the final location and rename\n",
        "part_files = glob.glob(f\"{temp_output_dir}/part-*.csv\")\n",
        "if len(part_files) != 1:\n",
        "    raise ValueError(\"Expected exactly one part file, but found multiple or none.\")\n",
        "shutil.move(part_files[0], final_output_path)\n",
        "\n",
        "# Remove the temporary directory\n",
        "shutil.rmtree(temp_output_dir)\n",
        "\n",
        "print(\"✅ Cleaned and sorted dataset saved as 'cleaned_data_V2.csv' in your Google Drive!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IVEZJOf_QbEo",
        "outputId": "8d897943-51c8-43e0-b607-1c9a2a1a10a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Cleaned & Sorted Data Preview:\n",
            "+----+----+----+----+----------+------+----+---+---+------+------+\n",
            "|   c|   h|   l|   o|         t|     v|   y|  m|  d|change|symbol|\n",
            "+----+----+----+----+----------+------+----+---+---+------+------+\n",
            "|1.66|1.66|1.66|1.66|03/04/2006|250000|2006|  4|  3|     0|   2GO|\n",
            "|1.64|1.64|1.64|1.64|04/04/2006| 13000|2006|  4|  4| -0.02|   2GO|\n",
            "|1.64|1.64| 1.6| 1.6|12/04/2006|320000|2006|  4| 12|     0|   2GO|\n",
            "|1.68|1.68|1.68|1.68|20/04/2006|  1000|2006|  4| 20|  0.04|   2GO|\n",
            "|1.68|1.68|1.68|1.68|21/04/2006|  3000|2006|  4| 21|     0|   2GO|\n",
            "|1.68|1.68|1.66|1.66|24/04/2006|  3000|2006|  4| 24|     0|   2GO|\n",
            "|1.66|1.68|1.66|1.68|25/04/2006| 23000|2006|  4| 25| -0.02|   2GO|\n",
            "|1.66|1.66|1.66|1.66|26/04/2006|  3000|2006|  4| 26|     0|   2GO|\n",
            "| 1.6|1.66| 1.6|1.66|27/04/2006| 76000|2006|  4| 27| -0.06|   2GO|\n",
            "|1.22|1.22|1.22|1.22|17/08/2006|  2000|2006|  8| 17| -0.38|   2GO|\n",
            "+----+----+----+----+----------+------+----+---+---+------+------+\n",
            "only showing top 10 rows\n",
            "\n",
            "✅ Cleaned and sorted dataset saved as 'cleaned_data_V2.csv' in your Google Drive!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install PySpark if not already installed\n",
        "!pip install -q pyspark\n",
        "\n",
        "# Import required libraries\n",
        "from pyspark.sql import SparkSession\n",
        "from google.colab import drive\n",
        "import os\n",
        "import shutil\n",
        "import glob\n",
        "\n",
        "# Step 1: Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Step 2: Start a Spark session\n",
        "spark = SparkSession.builder.appName(\"Data Preprocessing\").getOrCreate()\n",
        "\n",
        "# Step 3: Define correct paths\n",
        "base_path = \"/content/drive/MyDrive/Big Data Final Project/Dataset\"\n",
        "input_file = f\"{base_path}/dataset_with_NULL_values/stocks_with_nulls.csv\"\n",
        "output_folder = f\"{base_path}/Cleaned_Dataset\"\n",
        "final_output_path = f\"{output_folder}/cleaned_stocks_V2.csv\"\n",
        "temp_output_dir = \"/content/cleaned_temp_output\"\n",
        "\n",
        "# Step 4: Validate input file\n",
        "if not os.path.exists(input_file):\n",
        "    raise FileNotFoundError(f\"Input file not found: {input_file}\")\n",
        "\n",
        "# Step 5: Load the dataset\n",
        "df = spark.read.option(\"header\", \"true\").csv(input_file)\n",
        "\n",
        "# Step 6: Drop rows with null values and duplicates\n",
        "df_cleaned = df.dropna().dropDuplicates()\n",
        "\n",
        "# Sort alphabetically by 'Stock Name'\n",
        "df_sorted = df_cleaned.orderBy(\"Stock Name\")\n",
        "\n",
        "# Step 7: Show sample of cleaned data\n",
        "print(\"Cleaned Data:\")\n",
        "df_sorted.show()\n",
        "print(f\"Cleaned Row Count: {df_sorted.count()}\")\n",
        "\n",
        "# Step 8: Save the cleaned data as a single CSV\n",
        "os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "# Write to temporary output directory\n",
        "df_sorted.coalesce(1).write.mode(\"overwrite\").option(\"header\", \"true\").csv(temp_output_dir)\n",
        "\n",
        "# Move the actual part file to the final location and rename\n",
        "part_files = glob.glob(f\"{temp_output_dir}/part-*.csv\")\n",
        "if len(part_files) != 1:\n",
        "    raise ValueError(\"Expected exactly one part file, but found multiple or none.\")\n",
        "shutil.move(part_files[0], final_output_path)\n",
        "\n",
        "# Remove temporary Spark output folder\n",
        "shutil.rmtree(temp_output_dir)\n",
        "\n",
        "print(\"✅ Cleaned dataset saved as a single CSV in your Google Drive!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "86rECl-VVCq7",
        "outputId": "66a18b74-2c36-4813-9a6b-e877a5341ee6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Cleaned Data:\n",
            "+-------------+----+------------+-----+-----+-----+-----+-------+-------+\n",
            "|   Stock Name|Code|        Date|Price| Open| High|  Low| Volume|Change%|\n",
            "+-------------+----+------------+-----+-----+-----+-----+-------+-------+\n",
            "|2GO Group Inc| 2GO|Aug 26, 2016|  7.3| 7.25|  7.3| 7.25|128.10K|  0.69%|\n",
            "|2GO Group Inc| 2GO|Nov 20, 2019| 10.1| 10.5| 10.5|   10| 17.70K|  1.00%|\n",
            "|2GO Group Inc| 2GO|Dec 20, 2018|14.18|14.12|14.48|14.02|356.90K|  1.14%|\n",
            "|2GO Group Inc| 2GO|Aug 19, 2015|   10| 9.48|   10| 9.48|803.10K|  5.49%|\n",
            "|2GO Group Inc| 2GO|Oct 02, 2015|  7.6|  7.9|  7.9| 7.55|318.50K| -3.18%|\n",
            "|2GO Group Inc| 2GO|Aug 09, 2017| 21.2| 22.3| 22.7| 20.4|579.00K| -7.02%|\n",
            "|2GO Group Inc| 2GO|Sep 14, 2016| 7.28| 7.19| 7.28| 7.14|108.00K|  1.11%|\n",
            "|2GO Group Inc| 2GO|Jan 09, 2020|  9.6| 9.52|  9.9| 9.52| 11.90K|  0.84%|\n",
            "|2GO Group Inc| 2GO|Aug 27, 2015|  9.2| 9.45|  9.9|  9.2|918.50K| -0.76%|\n",
            "|2GO Group Inc| 2GO|Dec 22, 2015|    7|    7|    7| 6.97| 43.20K|  0.00%|\n",
            "|2GO Group Inc| 2GO|Nov 28, 2016| 7.55| 7.65| 7.65| 7.55| 19.00K| -1.31%|\n",
            "|2GO Group Inc| 2GO|Jul 30, 2020| 8.49| 8.38| 8.51| 8.12| 36.50K|  4.56%|\n",
            "|2GO Group Inc| 2GO|Feb 11, 2021|  8.6|  8.4|  9.1|  8.3|160.60K|  2.38%|\n",
            "|2GO Group Inc| 2GO|Feb 09, 2015| 4.44| 4.18| 4.55| 4.18|  1.17M|  7.77%|\n",
            "|2GO Group Inc| 2GO|Sep 15, 2017|20.35| 20.6|20.65|20.35| 55.10K| -1.45%|\n",
            "|2GO Group Inc| 2GO|Jul 21, 2016|  7.5| 7.73| 7.76| 7.42|334.80K| -2.98%|\n",
            "|2GO Group Inc| 2GO|Feb 05, 2021| 8.58| 8.45|  8.6|  8.3|  9.40K| -0.23%|\n",
            "|2GO Group Inc| 2GO|Jan 27, 2014|  1.9| 1.88|  1.9| 1.88|  3.00K|  5.56%|\n",
            "|2GO Group Inc| 2GO|Aug 31, 2016| 7.23| 7.25| 7.25| 7.19| 77.00K| -0.55%|\n",
            "|2GO Group Inc| 2GO|May 07, 2014|    3| 2.91|    3| 2.91| 13.00K| -0.66%|\n",
            "+-------------+----+------------+-----+-----+-----+-----+-------+-------+\n",
            "only showing top 20 rows\n",
            "\n",
            "Cleaned Row Count: 481920\n",
            "✅ Cleaned dataset saved as a single CSV in your Google Drive!\n"
          ]
        }
      ]
    }
  ]
}